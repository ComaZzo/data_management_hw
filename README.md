# data_management_hw
# Домашнее задание №1

## Зайти на Кинопоиск, найти 5 любимых фильмов и сделать по ним таблички с данными.

Табличка films:
- title - название фильма (текст)
- id (число) соответствует film_id в табличке persons2content
- country страна (тест)
- box_office сборы в долларах (число)
- release_date дата выпуска (date)

Табличка persons (актёры, режиссёры и т.д.)
- id (число) - соответствует person_id в табличке persons2content
- fio (текст) фамилия, имя

Табличка persons2content
- person_id (число) - id персоны
- film_id (число) - id контента
- person_type (текст) тип персоны (актёр, режиссёр и т.д.)

Таким образом реализуется схема БД "Звезда" с центром в табличке persons2content

**Важное замечание** Для этой домашки Postgres устанавливать не нужно, все запросы можно отладить на сайте http://sqlfiddle.com/

**Примечание** ответ должен быть предоставлен в виде файла формата *.sql*  нужными командами `CREATE TABLE`, `INSERT` и т.д.
# Домашнее задание №2

## 1. Простые выборки

- 1.1 SELECT , LIMIT - выбрать 10 записей из таблицы ratings (Для всех дальнейших запросов выбирать по 10 записей, если не указано иное)
- 1.2 WHERE, LIKE - выбрать из таблицы links всё записи, у которых imdbid оканчивается на "42", а поле movieid между 100 и 1000

## 2. Сложные выборки: JOIN

- 2.1 INNER JOIN выбрать из таблицы links все imdbId, которым ставили рейтинг 5

## 3. Аггрегация данных: базовые статистики

- 3.1 COUNT() Посчитать число фильмов без оценок
- 3.2 GROUP BY, HAVING вывести top-10 пользователей, у который средний рейтинг выше 3.5

## 4. Иерархические запросы

- 4.1 Подзапросы: достать любые 10 imbdId из links у которых средний рейтинг больше 3.5.
- 4.2 Common Table Expressions: посчитать средний рейтинг по пользователям, у которых более 10 оценок.  Нужно подсчитать средний рейтинг по все пользователям, которые попали под условие - то есть в ответе должно быть одно число.

## Как сдавать домашку

Выполненное домашнее задание - это файл формата .sql. Создать такой файл можно в любом текстовом редакторе - под Виндой советую Motepad++, под Linux Sublime.
Идеальный вариант - установить среду разработки Pycharm и все файлы открывать и редактировать в ней.
Шаблон файла можно найти в директории storage_backend/src/hw1.sql

Вот пример файл, который я создал hw1.sql
<pre>
SELECT 'ФИО: Капитан Америка';
-- первый запрос
SELECT * FROM ratings LIMIT 10;

-- второй запрос
SELECT userId, COUNT(*) FROM ratings GROUP BY 1 ORDER BY 2 DESC LIMIT 10;
</pre>

Первый запрос выводит ваше ФИО - его обязательно нужно внести, иначе домашка не будет зачтена.

Символами '--' отделяются коментарии в коде - это "пометки на полях", которые служат тому, чтобы код был более понятным.

Все 4 запроса домашки нужно оформить в этом файле.

Теперь можно запустить .sql файл с домашней работой с помощью psql
<pre>
psql -U postgres -f hw1.sql
</pre>

Результат работы скрипта
<pre>
 userid | movieid | rating | timestamp
--------+---------+--------+------------
      1 |     110 |      1 | 1425941529
      1 |     147 |    4.5 | 1425942435
      1 |     858 |      5 | 1425941523
      1 |    1221 |      5 | 1425941546
      1 |    1246 |      5 | 1425941556
      1 |    1968 |      4 | 1425942148
      1 |    2762 |    4.5 | 1425941300
      1 |    2918 |      5 | 1425941593
      1 |    2959 |      4 | 1425941601
      1 |    4226 |      4 | 1425942228
(10 rows)

 userid | count
--------+-------
  45811 | 36552
   8659 | 18558
 270123 | 15276
 179792 | 15030
 228291 | 14820
 243443 | 12640
  98415 | 12188
 229879 | 12048
  98787 | 11628
 172224 | 11402
(10 rows)
</pre>

В качестве ответа нужно прислать файл hw1.sql (лучше ссылку на github) и скриншот с экраном, где исполняются запросы.

# Домашнее задание #3

## Оконные функции.

Вывести список пользователей в формате userId, movieId, normed_rating, avg_rating где

- userId, movieId - без изменения
- для каждого пользователя преобразовать рейтинг `r` в нормированный `normed_rating=(r - r_min)/(r_max - r_min)`, где
    - `r_min` минимальное значение рейтинга у данного пользователя
    - `r_max` максимальное значение рейтинга у данного пользователя
- `avg_rating` - среднее значение рейтинга у данного пользователя

Вывести первые 30 таких записей

Ответ на задание - файл на github  расширением `.sql`.

# Домашнее задание #4

### ETL

ETL - процесс выгрузки данных, обработки и их дальнейшей загрузки. В рамках домашней работы нужно проделать все три этапа

#### Extract

Проверьте, что в директории `data_store` присутствует файл с мета-информацией по фильмам

```shell script
ls data_store/raw_data | grep movies_metadata.csv
```

Наша задача - загрузить это файл в Postgres. Для начала откройте его в excel и найдите столбец `genres`.
Каждая строка содержит JSON - в таком виде в Postgres залить не получится, поэтому для начала трансформируем столбец в "плоский" вид.

```shell script
python3 data_tools/extract_zipped_data.py -s transform
```

Результат работы скрипта - получили файл `data_store/raw_data/genres.csv`. Можно заливать его в Postgres!

Для начала подключимся в сеанс psql - тут нужно будет с помощью CREATE TABLE создать табличку `content_genres` для файла `genres.csv`.

```shell script
python3 upstart.py -s psql
``` 

Напишите команду создания таблички `content_genres` у неё должно быть 2 поля - `movieId` (числовой) и `genre` (текстовое).  Пример такой команды можно подсмотреть в файле [create_tables.sql](../docker_compose/postgres_host/create_tables.sql)
```sql
CREATE TABLE ...
```

Напишите команду копирования данных из файла в созданную вами таблицу - воспользуйтесь командой [copy](https://github.com/adzhumurat/data_management/blob/master/slides/postgres_db.md#data-importexport)
```sql
\copy
```

Подключитесь к контейнеру
<pre>
python3 upstart.py -s psql
</pre>

Проверьте, что в таблице есть записи
```sql
SELECT COUNT(*) FROM movie.genres;
```

Результат запроса
<pre>
count
-------
46419
</pre>

#### Transform

Мы загрузили данные в табличку, теперь нужно их преобразовать для дальнейшего использования. Мы ходитм узнать, какие теги у фильмов, которые сильно нравятся пользователям.

- Сформируйте запрос (назовём его ЗАПРОС1) к таблице ratings, в котором будут 2 поля
-- movieId
-- avg_rating - средний рейтинг, который ставят этому контенту пользователи
В выборку должны попасть те фильмы, которым поставили оценки более чем 50 пользователей
Список должен быть отсортирован по убыванию по полю avg_rating и по возрастанию по полю movieId
Из этой выборки оставить первые 150 элементов

Теперь мы хотим добавить к выборке хороших фильмов с высоким рейтингов информацию о тегах. Воспользуемся Common Table Expressions. Для этого нужно написать ЗАПРОС2, который присоединяет к выборке таблицу keywords

```sql
WITH top_rated as ( ЗАПРОС1 ) ЗАПРОС2;
```

#### Load

Мы обогатили выборку популярного контента внешними данными о тегах. Теперь мы можем сохранить эту информацию в таблицу для дальнейшего использования

Сохраним нашу выборку в новую таблицу top_rated_tags. Для этого мы модифицируем ЗАПРОС2 - вместо простого SELECT сделаем SELECT INTO.

Назовём всю эту конструкцию ЗАПРОС3
```sql
WITH top_rated as ( ЗАПРОС1 )  SELECT movieId, top_rated_tags INTO имя_таблицы FROM top_rated ...;
```

Теперь можно выгрузить таблицу в текстовый файл - [пример см. в лекции](https://github.com/adzhumurat/data_management/blob/master/slides/postgres_db.md#data-importexport).
Внимание: Поля в текстовом файле нужно разделить при помощи табуляции ( символ E`\t`).

Путь до файла с выгрузкой должен начинаться `/usr/share/data_store/raw_data/` - таким образом данные сохраняться в файловую систему вашей ОС, а не докера.

Решением домашки будет скрипт hw3.sql вида:

<pre>
"ВАША КОМАНДА СОЗДАНИЯ ТАБЛИЦЫ";

"ВАША КОМАНДА ЗАЛИВКИ ДАННЫХ В ТАБЛИЦу";

"ЗАПРОС3";

"ВАША КОМАНДА ВЫГРУЗКИ ТАБЛИЦЫ В ФАЙЛ"
</pre>
# Домашнее задание № 5.

Задание посвящено MongoDB. В рамках домашней работы необходимо:

- подключиться к Mongo из командной строки Linux и загрузить в Mongo текстовый JSON-файл `raw_data\tags.json`;
- выполнить запросы к Mongo через консоль:
    - подсчитайте число элементов в созданной коллекции tags в bd movies
    - подсчитайте число фильмов с конкретным тегом - `Adventure`
- используя группировку данных ($groupby) вывести top-3 самых распространённых тегов
- перенести запросы в файл [agg.js](https://github.com/adzhumurat/data_management/blob/master/docker_compose/data_client/app/src/agg.js)

Совет: запустите интерактивную консоль Mongo и выполните отладку запросов в консоли, а потом перенесите конструкции в agg.js.

# Сервис агрегации пользовательских оценок

## Идея сервиса

Среди данных в наборе [The Movies Dataset](https://www.kaggle.com/rounakbanik/the-movies-dataset) присутствует файл `ratings.csv`.
Этот файл хранит информацию об оценках пользователей в виде "плоской" структуры, где каждая строчка файла содержит событие вида: пользователь с идентификатором `uid_i` проставил контенту `cid_j` оценку `r_ij` в момент времени `t_ij`

| userId | movieId | rating | timestamp |
| --- | --- | --- | --- |
| uid_1 | mid_11 | r_11 | ts_11 |
| uid_1 | mid_12 | r_12 | ts_12 |
| uid_1 | mid_13 | r_13 | ts_13 |
| ... | ... | ... | ... |
| uid_n | mid_n1 | r_n1 | ts_n1 |
| uid_n | mid_n2 | r_n2 | ts_n2 |
| uid_n | mid_n3 | r_n3 | ts_n3 |
| uid_n | mid_n4 | r_n4 | ts_n4 |

Примерно в таком виде хранит пользовательский фидбэк [онлайн-кинотеатр ivi](https://www.ivi.ru/). Плюс такой структуры хранения данных, когда отдельному логическому событию соответствует одна запись в источнике данных, состоит в "атомарности" хранения - например, вы можете очень быстро вычистить записи по конкретным `uid` если узнаете, что эти оценки принадлежат ботам и являются результатом накрутки. Или, например, можно быстро "схлопнуть" задвоенные записи, которые являются в результатом сбоя.

В рамках pet-project предлагается создать сервис по агрегции пользовательских оценок, который будет представлять собой простое API, принимающее запросы вида
<pre>
curl -s "http://youservice/rates/uid"
</pre>

Для каждого переданного uid сервис будет выводить историю оценок, которые ставил этот пользователь, в виде JSON
<pre>
[
    {"movie_id": 4119470, "rating": 4, "timestamp": "2019-09-03 10:00:00"},
    {"movie_id": 5691170, "rating": 2, "timestamp": "2019-09-05 13:23:00"},
    {"movie_id": 3341191, "rating": 5, "timestamp": "2019-09-08 16:40:00"}
]
</pre>

## Детали реализации

Проект должен использовать технологию виртуалиции Docker и включать в себя несколько компонентов

* http-сервер на Python для ответа на внешние запросы
* Postgres для долгосрочного хранения данных из файла `ratings.csv`
* Redis для кеширования запросов в Postgres

Компоненты должны соединяться с помощью конфигурационного файла `docker-compose.yml`.

Пайплайн работы сервиса:

* с помощью docker-compose поднимаем Postgres. Создаём в базе таблицу ratings
* монтируем в контейнер с Postgres внешнюю директорию (пустую) `pg_data` для файлов БД Postgres
* монтируем в контейнер с http-сервером монтируем внешнюю директорию, которая содержит файл `ratings.csv`
* в контейнере с http-сервером на Python запускаем загрузку данных `\\copy ratings FROM '/data/ratings.csv' DELIMITER ',' CSV HEADER`
* с помощью docker-compose поднимаем контейнер с Redis - туда будем кешировать результаты запросов к Postgres
* запускаем http-сервер, порт из контейнера должен быть прокинут в хост-машину, на которой запускается докер 

Когда проект развернут, алгоритм работы следующий

* http-сервер принимает запрос, достаём из запроса user_id
* проверяем, есть ли в Redis результаты истории смотрения этого пользователя. Если есть - отвечаем на запрос
* если данных в кеше нет - инициируем запрос к Postgres, кладём результаты в Redis

Таким образом в результате этого проекта будет реализован сервис агрегации пользовательских оценок на базе Postgres с кеширующим сервером Redis.

# Реализация

Код проекта вы найдёте в файле [simple_service.py](../../docker_compose/data_client/app/src/simple_service.py)

* старт веб-сервиса  ` python3 upstart.py -s service`
* с помощью `docker ps` убедитесь, что запустился контейнер `data-mng_servce`
* убедитесь что сервис работает, открыв в браузере `http://localhost:5001/ping`
* Убедитесь, что сервер подключился к Postgres, открыв в браузере `http://localhost:5001/user/profile/1`
* подключитесь к мониторингу Redis `docker exec -it 876ff73a6c7f "redis-cli" monitor`
* подключитесь к мониторингу Postgres `docker logs -f $(docker ps -qf "name=postgres_host")`

